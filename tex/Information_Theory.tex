\documentclass[]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}

%Here I define some theorem styles and shortcut commands for symbols I use often
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem*{rmk}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem*{joke}{Joke}
\newtheorem{ex}{Example}
\newtheorem*{soln}{Solution}
\newtheorem{prop}{Proposition}

%Pagination stuff.
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\pagestyle{empty}



\begin{document}


\begin{center}
{\Large New York Yankees Statistical Analysis Questionnaire}\\
Michael Hirsch\\ %name here
January 26, 2015 %date here
\end{center}

\vspace{0.2 cm}


\subsection*{}


\begin{enumerate}
\item\label{norms}

1.	What perks do you expect from working for the Yankees?

\begin{proof}[\unskip\nopunct]\renewcommand{\qedsymbol}{}
	I would expect employee benefits including health and commuter benefits from any organization that employees me. With regards to perks that are specific to the Yankees, I would expect that I would receive discounts on tickets and merchandise.
\end{proof}

\item

What are your options for reducing these space requirements?

\begin{proof}[\unskip\nopunct]
Assuming that each sequence was stored in binary and that each occurrence of black or white was stored using 1 bit, for example, the 10 sequence consisting of ``W,W,W,W,W,W,W,W,B,W'' encoded as 0000000010, then we can instead encode n-tuples. For example:

$$White, White \rightarrow 0$$
$$Black, White \rightarrow 10$$
$$White, Black \rightarrow 100$$
$$Black, Black \rightarrow 111$$

So our original sequence can be encoded as 0000100, instead of 0000000010, saving us 3 bits. This is just one example, and there are definitely more efficient ways of doing this for sequences of length 100. Actually, in the case (albeit very rare) of a string of 10 black pixels, this compression algorithm is \textit{less} effective, requiring 30 bits to encode. However, this will be better in most cases.


% As we can see from the above calculation, pixel sequences that contain 3 black pixels occur overwhelmingly more often than the other sequences. Therefore, we can limit these sequences to by including only more probable sequences.

% Since the probability of a black pixel is quite low, we can deduce that uncertainty of the sequence is quanitified in a lower entropy.

% $$H(X) = - \log 00.05^{0.005} (0.995)^{0.995} = 0.005 \log \dfrac{e}{0.005} \approx 0.03147$$

% Since $t = 161700$ is quite large, it is possible to code these sequences in $2^{Ht}$ encoded sequences. That is:

% $$2^{Ht} = 2^{0.03147*161700} = $$
% The probability that we will encounter a sequence with precisely 3 black pixels is quite low:

% $${100 \choose 3}(0.005)^{3}(0.995)^{97} \approx 0.0125 $$

% Which means that  a large amount ($161700/166751 = 0.9697$)  of our table is taken up by an encoding for sequences that have only a probability of $0.0125$ of occuring. So, we can save space by not encoding these sequences, and accept that we will get an error in these cases as well. In doing so, we are saving space at a loss of certainty, now encoding about 98.6\% of sequences:

% $$\sum_{i=0}^{2} {100 \choose i}(0.005)^{i}(0.995)^{100-i} = 0.6057 + 0.3044 + 0.0757 = 0.9858$$

% However, if we want to store all sequences, we can use the Source Coding Theorem to econde all of them using less information. We calculate the entropy of our system:

% $$H(X) = - \log 0.005^{0.005} (0.995)^{0.995} = 0.005 \log \dfrac{e}{0.005} \approx 0.03147$$

% then multiply this by the number of sequences we are coding:

% $$0.03147*166751 = 5248 $$
\end{proof}

\item	Bound the probability that this encoding scheme will encounter an untabulated sequence.

\begin{proof}[\unskip\nopunct]
		
If we take $Black = 1$ and $White = 0$ then $E[X_{i}] = 0.005$ and $Var[X_{i}] = (0.005)(0.995) = 0.004975$ for each random variable $X_{i}$

We look at the random variable $S_{n}$ which is the sum of precisely $n$  $X_{i}$ random variables. We will not have codes for sequences where $S_{100} \geq 4$. However, Chebyshev's inequality has $|S_{n} - nE[X]| \geq \epsilon$ so we must take $\epsilon = 3.5$, since $nE[X] = 100(0.005) = 0.5$

$$Pr\{S_{100} \geq 4\} \leq \dfrac{100(0.005)(0.995)}{3.5^{2}} \approx 0.0406$$

This is about 23 times larger than the actual value of 0.0017

\end{proof}

\end{enumerate}




\end{document}


